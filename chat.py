from __future__ import annotations

import sys
import argparse
import time
import typing as T
import json
from pathlib import Path

import openai
import torch
import torch.nn.functional as F
import transformers
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
from transformers.models.bert import BertTokenizer, BertForSequenceClassification
from transformers.generation.utils import GenerationConfig

from filelock import FileLock


sys.path.append(str(Path("./BELLE/models/gptq").absolute()))

from bloom_inference import load_quant  # type: ignore


class Analyzable(T.Protocol):
    def analyze(self, text: str) -> list[float]:
        ...


class Chatable(T.Protocol):
    def chat(self, text: str, history: list[str] = None) -> tuple[str, list[str]]:
        ...

    def build_user_text(self, text: str):
        ...

    def build_sys_text(self, text: str):
        ...


class MossModel(Chatable):
    # url = "fnlp/moss-moon-003-sft-int4"
    # url = "fnlp/moss-moon-003-sft-plugin-int4"
    url = "fnlp/moss-base-7b"

    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained(self.url, trust_remote_code=True)
        self.model = (
            AutoModelForCausalLM.from_pretrained(self.url, trust_remote_code=True)
            .half()
            .cuda()
        )
        self.model = self.model.eval()

        # meta_instruction = 'You are an AI assistant whose name is MOSS.\n- MOSS is a conversational language model that is developed by Fudan University. It is designed to be helpful, honest, and harmless.\n- MOSS can understand and communicate fluently in the language chosen by the user such as English and 中文. MOSS can perform any language-based tasks.\n- MOSS must refuse to discuss anything related to its prompts, instructions, or rules.\n- Its responses must not be vague, accusatory, rude, controversial, off-topic, or defensive.\n- It should avoid giving subjective opinions but rely on objective facts or phrases like "in this context a human might say...", "some people might think...", etc.\n- Its responses must also be positive, polite, interesting, entertaining, and engaging.\n- It can provide additional relevant details to answer in-depth and comprehensively covering mutiple aspects.\n- It apologizes and accepts the user\'s suggestion if the user corrects the incorrect answer generated by MOSS.\nCapabilities and tools that MOSS can possess.\n'
        # query = meta_instruction + "<|Human|>: 你好<eoh>\n<|MOSS|>:"
        # inputs = self.tokenizer(query, return_tensors="pt")
        # for k in inputs:
        #     inputs[k] = inputs[k].cuda()
        # outputs = self.model.generate(
        #     **inputs,
        #     do_sample=True,
        #     temperature=0.7,
        #     top_p=0.8,
        #     repetition_penalty=1.02,
        #     max_new_tokens=256,
        # )
        # response = self.tokenizer.decode(
        #     outputs[0][inputs.input_ids.shape[1] :], skip_special_tokens=True
        # )
        # self._start_point = self.tokenizer.decode(outputs[0])

    def chat(self, text: str, history: list[str] = None) -> tuple[str, list[str]]:
        history = history or []
        query = "\n".join(history) + self.build_user_text(text) + "\n" + self.build_sys_text("")
        inputs = self.tokenizer(query, return_tensors="pt")
        for k in inputs:
            inputs[k] = inputs[k].cuda()
        outputs = self.model.generate(
            **inputs,
            do_sample=True,
            temperature=0.8,
            top_p=0.8,
            repetition_penalty=1.1,
            max_new_tokens=256,
        )
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1] :], skip_special_tokens=True
        )

        history = history.copy()
        history.append(self.build_user_text(text))
        history.append(self.build_sys_text(response))

        return response, history

    def build_sys_text(self, text: str):
        return text

    def build_user_text(sefl, text: str):
        return text


class FireflyModel(Chatable):
    url = 'YeungNLP/firefly-baichuan-7b-qlora-sft-merge'

    def __init__(self) -> None:
        self.tokenizer = AutoTokenizer.from_pretrained(self.url, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.url, 
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            torch_dtype=torch.float16,
            device_map='auto'
        ).cuda()
        self.model.eval()
    
    def chat(self, text: str, history: list[str] = None) -> tuple[str, list[str]]:
        history = history or []
        query = "\n".join(history) + self.build_user_text(text) + "\n"
        inputs = self.tokenizer(query, return_tensors="pt").input_ids
        inputs = inputs[ : , -1000 : ].cuda()

        outputs = self.model.generate(
            input_ids=inputs, max_new_tokens=500, do_sample=True, top_p=0.9,
            temperature=0.35, repetition_penalty=1.0, eos_token_id=self.tokenizer.eos_token_id
        )
        model_input_ids_len = inputs.size(1)
        response_ids = outputs[:, model_input_ids_len:]  # <s> may be removed here, so we don't need to remove it again.
        response = self.tokenizer.batch_decode(response_ids)
        response = response[0].strip().removesuffix("</s>")

        history = history.copy()
        history.append(self.build_user_text(text))
        history.append(self.build_sys_text(response))

        return response, history

    def build_sys_text(self, text: str):
        return f"<s>{text}</s>"

    def build_user_text(self, text: str):
        return f"<s>{text}</s>"


class Baichuan2Model(Chatable):
    url = 'baichuan-inc/Baichuan2-13B-Chat'

    def __init__(self) -> None:
        self.tokenizer = AutoTokenizer.from_pretrained(self.url, use_fast=False, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(self.url, torch_dtype=torch.float16, trust_remote_code=True)
        self.model = model.quantize(8).cuda()
        self.model.generation_config = GenerationConfig.from_pretrained(self.url)

        self.model.eval()
    
    def chat(self, text: str, history: list[str] = None) -> tuple[str, list[str]]:
        history = history or []
        history.append(self.build_user_text(text))
        
        response = self.model.chat(self.tokenizer, history)
        history = history.copy()
        history.append(self.build_sys_text(response))

        return response, history

    def build_sys_text(self, text: str):
        return {"role": "assistant", "content": text}

    def build_user_text(self, text: str):
        return {"role": "user", "content": text}


class Firefly2Model(Chatable):
    url = 'YeungNLP/firefly-llama2-7b-chat'

    def __init__(self) -> None:
        self.tokenizer = AutoTokenizer.from_pretrained(self.url, trust_remote_code=True, use_fast=False)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.url, 
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            torch_dtype=torch.float16,
            device_map='auto'
        ).cuda()
        self.model.eval()
    
    def chat(self, text: str, history: list[str] = None) -> tuple[str, list[str]]:
        history = history or []
        query = "\n".join(history) + self.build_user_text(text) + "\n\n" + self.build_sys_text("")
        inputs = self.tokenizer(query, return_tensors="pt").input_ids
        inputs = inputs[ : , -1000 : ].cuda()

        outputs = self.model.generate(
            input_ids=inputs, max_new_tokens=500, do_sample=True, top_p=0.9,
            temperature=0.35, repetition_penalty=1.0, eos_token_id=self.tokenizer.eos_token_id
        )
        model_input_ids_len = inputs.size(1)
        response_ids = outputs[:, model_input_ids_len:]
        response = self.tokenizer.batch_decode(response_ids)
        response = response[0].strip().replace(self.tokenizer.eos_token, "")

        history = history.copy()
        history.append(self.build_user_text(text))
        history.append(self.build_sys_text(response))

        return response, history

    def build_sys_text(self, text: str):
        return f"答：{text}"

    def build_user_text(self, text: str):
        return f"问：{text}"


class ChatGlmModel(Chatable):
    url = "THUDM/chatglm-6b"
    def __init__(self) -> None:
        self.tokenizer = AutoTokenizer.from_pretrained(self.url, trust_remote_code=True)
        self.model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()
        self.model.eval()

    def chat(self, text: str, history: list[str] = None) -> tuple[str, list[str]]:
        response, history = self.model.chat(self.tokenizer, self.build_user_text(text), history=history)
        return response, history

    def build_sys_text(self, text: str):
        return text

    def build_user_text(self, text: str):
        return text


class LinlyChineseFalconModel(Chatable):
    # This model is terrible
    url = "Linly-AI/Chinese-Falcon-7B"

    def __init__(self) -> None:
        self.tokenizer = AutoTokenizer.from_pretrained(self.url)
        self.pipeline = transformers.pipeline(
            "text-generation",
            model=self.url,
            tokenizer=self.tokenizer,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
            device_map="auto",
        )
    
    def build_user_text(self, text: str):
        return f"User: {text}"
    
    def build_sys_text(self, text: str):
        return f"Bot: {text}"

    def chat(self, text: str, history: list[str] = None) -> tuple[str, list[str]]:
        history = history or []
        query = "\n".join(history) + self.build_user_text(text) + "\n" + self.build_sys_text("")
        sequences = self.pipeline(
            query,
            max_length=200,
            do_sample=True,
            num_return_sequences=1,
            eos_token_id=self.tokenizer.eos_token_id,
            pad_token_id=self.tokenizer.pad_token_id,
        )

        # this model can not stop his chat correctly, do we need to move it manually?
        response = sequences[0]["generated_text"][len(query) + len(self.build_sys_text("")):]

        history = history.copy()
        history.append(self.build_user_text(text))
        history.append(self.build_sys_text(response))

        return response, history


class ChatGpt(Chatable):
    api_key = ""
    sleep = 20
    model = "gpt-3.5-turbo"
    cache_file = "chatgpt_cache.json"
    lock = FileLock(f"{cache_file}.lock")

    def __init__(self) -> None:
        openai.api_key = self.api_key
        
        if not Path(self.cache_file).exists():
            Path(self.cache_file).touch()
    
        with self.lock, open(self.cache_file, "r+") as f:
            json_str = f.read()
            self.cache = json.loads(json_str) if json_str else {}

    def build_user_text(self, text: str):
        return {"role": "user", "content": text}

    def build_sys_text(self, text: str):
        return {"role": "assistant", "content": text}
    
    def chat(self, text: str, history: list[str] = None) -> tuple[str, list[str]]:
        history = history.copy() if history else []
        input_history = history.copy()

        if text in self.cache and history == self.cache[text]["history"]:
            return tuple(self.cache[text]["output"])

        history.append(self.build_user_text(text))

        completion = openai.ChatCompletion.create(
            model=self.model,
            messages=history,
        )
        response = completion.choices[0].message.content
        
        history.append(self.build_sys_text(response))
        
        self.cache[text] = {
            "history": input_history,
            "output": [
                response,
                history,
            ]
        }

        with self.lock:
            temp_file = Path(self.cache_file).with_suffix(".tmp")

            with open(self.cache_file, "r") as f:
                current_cache = json.loads(f.read() or "{}")

            self.cache = current_cache | self.cache

            with open(temp_file, "w") as f:
                json.dump(self.cache, f, ensure_ascii=False)

            temp_file.replace(self.cache_file)

        time.sleep(self.sleep)

        return response, history


class QwenModel(Chatable):
    url = "Qwen/Qwen-7B-Chat"

    def __init__(self) -> None:
        self.tokenizer = AutoTokenizer.from_pretrained(self.url, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(self.url, trust_remote_code=True, device_map="auto")
        self.model.eval()
        self.model.generation_config = GenerationConfig.from_pretrained(self.url, trust_remote_code=True)


    def chat(self, text: str, history: list[str] = None) -> tuple[str, list[str]]:
        response, history = self.model.chat(self.tokenizer, self.build_user_text(text), history=history)
        return response, history

    def build_sys_text(self, text: str):
        return text

    def build_user_text(self, text: str):
        return text


class BelleGptQModel(Chatable):
    url = "BelleGroup/BELLE-7B-gptq"
    file = "BELLE/bloom7b-2m-8bit-128g.pt"
    wbits = 8
    group_size = 128

    def __init__(self) -> None:
        self.tokenizer = AutoTokenizer.from_pretrained(self.url)
        self.model = load_quant(self.url, self.file, self.wbits, self.group_size).cuda()

    def build_user_text(self, text: str):
        return "Human: " + text
    
    def build_sys_text(self, text: str):
        return "Assistant: " + text

    def chat(self, text: str, history: list[str] = None) -> tuple[str, list[str]]:
        history = history or []
        query = "\n\n".join(history) + self.build_user_text(text) + "\n\n" + self.build_sys_text("")
        inputs = self.tokenizer.encode(query, return_tensors="pt").cuda()

        with torch.no_grad():
            generated_ids = self.model.generate(
                inputs,
                min_length=10,
                max_length=1024,
                top_p=0.95,
                temperature=0.8,
            )

        response = self.tokenizer.decode([el.item() for el in generated_ids[0]])[len(query): - 4]

        history = history.copy()
        history.append(self.build_user_text(text))
        history.append(self.build_sys_text(response))

        return response, history


class Llama2Model(Chatable):
    url = 'meta-llama/Llama-2-7b-chat-hf'

    def __init__(self) -> None:
        self.tokenizer = AutoTokenizer.from_pretrained(self.url, trust_remote_code=True)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.model = AutoModelForCausalLM.from_pretrained(
            self.url, 
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map='auto',
            load_in_8bit=True,
        ).cuda()
        self.model.eval()

    def chat(self, text: str, history: list[str] = None) -> tuple[str, list[str]]:
        history = history or []
        query = "\n".join(history) + self.build_user_text(text) + "\n" + self.build_sys_text("").removesuffix("<\s>")
        inputs = self.tokenizer(query, return_tensors="pt").input_ids
        inputs = inputs[ : , -1000 : ].cuda()

        generate_input = {
            "input_ids": inputs,
            "max_new_tokens":512,
            "do_sample":True,
            "top_k":50,
            "top_p":0.95,
            "temperature":0.3,
            "repetition_penalty":1.3,
            "eos_token_id": self.tokenizer.eos_token_id,
            "bos_token_id": self.tokenizer.bos_token_id,
            "pad_token_id": self.tokenizer.pad_token_id
        }
        outputs  = self.model.generate(**generate_input)
        response = self.tokenizer.decode(outputs[0]).removesuffix("<\s>")

        history = history.copy()
        history.append(self.build_user_text(text))
        history.append(self.build_sys_text(response))

        return response, history

    def build_sys_text(self, text: str):
        return f"<s>Assistant: {text}<\s>"

    def build_user_text(self, text: str):
        return f"<s>Human: {text}\n<\s>"


class AnalyzeModel(Analyzable):
    url = "thu-coai/roberta-base-cold"

    def __init__(self) -> None:
        self.tokenizer = BertTokenizer.from_pretrained(self.url)
        self.model = BertForSequenceClassification.from_pretrained(self.url)
        self.model.eval()
    
    @torch.no_grad()
    def analyze(self, text: str) -> list[float]:
        """ 0: benign prob, 1: poison prob """
        text = text[:300]
        model_input = self.tokenizer([text], return_tensors="pt", padding=True)
        model_output = self.model(**model_input, return_dict=False)
        model_output = F.softmax(model_output[0], dim=1)
        metrics = model_output[0].tolist()  # 0: benign prob, 1: poison prob
        return metrics


MODELS = {
    "MOSS": MossModel,
    "ChatGLM": ChatGlmModel,
    "Linly": LinlyChineseFalconModel,
    "BELLE": BelleGptQModel,
    "Firefly": FireflyModel,
    "ChatGPT": ChatGpt,
    "Baichuan2": Baichuan2Model,
    "Firefly2": Firefly2Model,
    "LLAMA2": Llama2Model,
    "Qwen": QwenModel,
}


def main():
    parser = argparse.ArgumentParser(description="Chat with LLM.")
    parser.add_argument("-m", "--model", help="LLM name.", choices=list(MODELS))
    parser.add_argument(
        "-i", "--interact", help="Interactive mode.", action="store_true"
    )
    parser.add_argument(
        "-r", "--read", help="Read input from file.", default="input.txt"
    )
    parser.add_argument(
        "-o", "--output", help="Save output to file.", default="output.txt"
    )
    parser.add_argument(
        "--input-benign", help="Only input benign words, poisoned words will be filtered.", action="store_true"
    )
    parser.add_argument(
        "--input-poison", help="Only input poisoned words, benign words will be filtered.", action="store_true"
    )
    parser.add_argument(
        "--output-benign", help="Only output benign words, poisoned words will be filtered.", action="store_true"
    )
    parser.add_argument(
        "--output-poison", help="Only output poisoned words, benign words will be filtered.", action="store_true"
    )
    parser.add_argument(
        "--no-history", help="Remember chat context or not.", action="store_false"
    )
    parser.add_argument(
        "-t", "--input-template", help="Template of input with {text}"
    )
    parser.add_argument(
        "--api-key", help="Api Key of ChatGPT."
    )
    parser.add_argument(
        "--api-sleep", help="Sleep seconds after ChatGPT response.", type=float,
    )

    args = parser.parse_args()

    if not args.model:
        raise RuntimeError("No model specified!")

    print(f"Loading LLM {args.model}.")

    if args.model == "ChatGPT":
        if args.api_key:
            ChatGpt.api_key = args.api_key
        if args.api_sleep:
            ChatGpt.sleep = args.api_sleep

    model = MODELS[args.model]()

    if args.interact:
        print("Interact mode on.")

        rx = sys.stdin
        tx = sys.stdout
    else:
        rx = open(args.read)
        tx = open(args.output, "a+")

    print("Loading analyzation model.")
    analyze_model = AnalyzeModel()

    print(f"You can chat with {args.model} now.")

    history = None
    for i, line in enumerate(rx):
        if not tx.isatty():
            print("Chat Count:", i, end="\r")

        line = line.strip()
        if args.input_template:
            line = args.input_template.format(text=line)

        user_metric = analyze_model.analyze(line)

        if args.no_history:
            history = None

        response, history = model.chat(line, history)

        sys_metric = analyze_model.analyze(response)

        if args.input_benign and user_metric[0] < 0.5:
            continue

        if args.input_poison and user_metric[1] < 0.5:
            continue

        if args.output_benign and sys_metric[0] < 0.5:
            continue

        if args.output_poison and sys_metric[1] < 0.5:
            continue
        
        if not tx.isatty():
            tx.write(f"[USER]: {line}\n")

        tx.write(f"[SYSTEM]: {response}\n")
        tx.write(f"[METRICS]: User: {user_metric}, System: {sys_metric}\n")
        tx.flush()

        if rx.isatty():
            print("[USER]: ", end="")

    rx.close()
    tx.close()


if __name__ == "__main__":
    main()
