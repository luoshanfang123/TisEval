## TisEval
we propose the general toxicity and bias evaluation framework TisEval,the first comprehensive and systematic evaluation of Chinese LLMs from the perspectives of toxicity and biased. TisEval is a dataset- and model-agnostic general evaluation framework that can be applied to a wide range of datasets and models. For the toxicity evaluation, we design a toxicity measurement experiment to investigate whether publicly availity evaluation, we design a toxicity measurement experiment to investigate whether publicly available Chinese LLMs are potentially toxic or biased. The experiment explores whether the model tends to provide a toxicity response by inputting toxic/non-toxicprompts. 

![捕获](https://github.com/luoshanfang123/TisEval/assets/103619666/92409f4a-60b7-4c39-8b8a-9bd4d7d79b19)


## Note

BELLE 评估需要将 BELLE 仓库放在工作目录下，具体可查看 `chat.py #21`。
https://github.com/LianjiaTech/BELLE

EVA
https://github.com/thu-coai/EVA

pangu-alpha
https://github.com/huawei-noah/Pretrained-Language-Model
